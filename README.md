# ğŸ›ï¸ Serverless AI Shopping Assistant (AWS Bedrock)

A scalable serverless AI backend using **AWS Lambda**, **Amazon Bedrock** (Titan embeddings & text), and **API Gateway**, delivering real-time Arabic responses via a **RAG-based pipeline**. Built with semantic search and intent detection for intelligent product recommendations.

**Project by:** [Narjis Bin Tayeb](https://www.linkedin.com/in/narjis-tayeb) | Applied Data Scientist specializing in LLMs, RAG systems, and Cloud Solutions

## ğŸŒŸ Features

- **ğŸ” Semantic Search**: Find products using natural language in Arabic or English
- **ğŸ¤– Intent Detection**: Automatically detects user intent (compare, budget, recommendations, etc.)
- **ğŸ’° Smart Filtering**: Filters by budget, category, and price range
- **ğŸ“Š Product Comparison**: Compare multiple products with detailed analysis
- **âš¡ Fast Response**: Pre-computed embeddings for millisecond-level search
- **ğŸŒ Bilingual Support**: Works with Arabic and English queries

## ğŸ—ï¸ Architecture

```
User Query â†’ API Gateway â†’ Lambda Function â†’ Bedrock (Titan)
                              â†“
                          S3 Bucket
                    (Embeddings + Metadata)
```

### Components:

1. **Pre-processing (Colab)**: Generate embeddings for 300+ products
2. **Storage (S3)**: Store embeddings and product metadata
3. **Runtime (Lambda)**: Handle queries, retrieve similar products, generate responses
4. **AI Models (Bedrock)**:
   - `amazon.titan-embed-text-v2:0` - Generate 1536-dim embeddings
   - `amazon.titan-text-express-v1` - Generate natural language responses

## ğŸš€ Quick Start

### Prerequisites

- AWS Account with Bedrock access
- Python 3.9+
- AWS CLI configured
- S3 bucket created

### 1. Generate Embeddings (One-time setup)

```python
# Run in Google Colab or local environment
import json, numpy as np, boto3, time

bucket_name = "ecommerce-ai-agent-storage"
products_file = "products_300.json"
model_id = "amazon.titan-embed-text-v2:0"

# Load products
with open(products_file, "r", encoding="utf-8") as f:
    products = json.load(f)

texts = [item["name"] + " " + item.get("description","") for item in products]

# Generate embeddings
bedrock = boto3.client("bedrock-runtime")
vectors = []

for i, text in enumerate(texts):
    response = bedrock.invoke_model(
        modelId=model_id,
        contentType="application/json",
        accept="application/json",
        body=json.dumps({"inputText": text})
    )
    vec = json.loads(response["body"].read())["embedding"]
    vectors.append(vec)
    time.sleep(0.1)  # Rate limiting

# Save files
embeddings = np.array(vectors)
np.save("embeddings.npy", embeddings)

with open("metadata.json", "w", encoding="utf-8") as f:
    json.dump(products, f, ensure_ascii=False)

# Upload to S3
s3 = boto3.client('s3')
s3.upload_file('embeddings.npy', bucket_name, 'embeddings.npy')
s3.upload_file('metadata.json', bucket_name, 'metadata.json')
```

### 2. Deploy Lambda Function

1. **Create Lambda function** (Python 3.9+)
2. **Copy the Lambda code** from `lambda_function.py`
3. **Configure**:
   - Memory: 512 MB
   - Timeout: 60 seconds
   - Runtime: Python 3.9

4. **Add IAM permissions**:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:::ecommerce-ai-agent-storage/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel"
      ],
      "Resource": "arn:aws:bedrock:*:*:foundation-model/*"
    }
  ]
}
```

### 3. Set Up API Gateway

1. Create HTTP API
2. Add POST route â†’ Lambda integration
3. Enable CORS
4. Deploy API

### 4. Test It!

```bash
curl -X POST https://your-api-id.execute-api.region.amazonaws.com/prod \
  -H "Content-Type: application/json" \
  -d '{"query": "Ø¹Ø·Ø± Ø±Ø¬Ø§Ù„ÙŠ ÙØ®Ù… ØªØ­Øª 500 Ø±ÙŠØ§Ù„"}'
```

## ğŸ“ API Reference

### Request

```json
POST /
{
  "query": "string"  // User's search query in Arabic or English
}
```

### Response

```json
{
  "query": "Ø¹Ø·Ø± Ø±Ø¬Ø§Ù„ÙŠ ÙØ®Ù… ØªØ­Øª 500 Ø±ÙŠØ§Ù„",
  "intent": "BUDGET",
  "answer": "Ø¥Ù„ÙŠÙƒ Ø£ÙØ¶Ù„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø¶Ù…Ù† Ù…ÙŠØ²Ø§Ù†ÙŠØªÙƒ...",
  "products": [
    {
      "id": "12345",
      "name": "Ø¹Ø·Ø± Ø¯ÙˆÙ„ØªØ´ÙŠ Ø¢Ù†Ø¯ ØºØ§Ø¨Ø§Ù†Ø§",
      "category": "Ø¹Ø·ÙˆØ±",
      "price": "450",
      "description": "...",
      "tags": ["Ø±Ø¬Ø§Ù„ÙŠ", "ÙØ®Ù…", "Ø®Ø´Ø¨ÙŠ"]
    }
  ]
}
```

## ğŸ¯ Supported Intents

| Intent | Example Queries | Behavior |
|--------|----------------|----------|
| **RECOMMEND** | "Ø¹Ø·Ø± Ø±Ø¬Ø§Ù„ÙŠ" | Returns top 3 relevant products |
| **COMPARE** | "Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø¹Ø·Ø± A Ùˆ B" | Compares multiple products |
| **BUDGET** | "ØªØ­Øª 500 Ø±ÙŠØ§Ù„" | Filters by price ceiling |
| **MAX_PRICE** | "Ø£ØºÙ„Ù‰ Ø¹Ø·Ø±" | Returns most expensive match |
| **MIN_PRICE** | "Ø£Ø±Ø®Øµ Ø®ÙŠØ§Ø±" | Returns cheapest match |

## ğŸ§  How It Works

### 1. **Embedding Generation** (Preprocessing)
```
Product Text â†’ Titan Embed V2 â†’ 1536-dim Vector â†’ S3
```

### 2. **Query Processing** (Runtime)
```
User Query â†’ Titan Embed V2 â†’ Query Vector
                                    â†“
            Cosine Similarity with Product Vectors
                                    â†“
                        Top 12 Candidates Retrieved
                                    â†“
                    Intent-Based Filtering + Ranking
                                    â†“
                          Final 3 Products Selected
                                    â†“
            Titan Text Express â†’ Natural Language Response
```

### 3. **Smart Selection Logic**

```python
# Example: Budget filtering
budget = extract_budget("ØªØ­Øª 500 Ø±ÙŠØ§Ù„")  # â†’ 500.0
filtered = [p for p in candidates if p["price"] <= budget]

# Example: Category filtering  
if is_perfume_query(query):
    filtered = [p for p in candidates if p["category"] == "Ø¹Ø·ÙˆØ±"]
```

## ğŸ“Š Performance

- **Cold Start**: ~3-5 seconds (loading embeddings from S3)
- **Warm Request**: ~1-2 seconds (embedding query + LLM generation)
- **Search Accuracy**: Semantic similarity using cosine distance
- **Cost**: ~$0.001 per query (Bedrock Titan pricing)

## ğŸ”§ Configuration

Key parameters in `lambda_function.py`:

```python
TOP_K = 12       # Candidates from RAG before filtering
FINAL_K = 3      # Final products in response
embed_model_id = "amazon.titan-embed-text-v2:0"
text_model_id = "amazon.titan-text-express-v1"
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ colab_embedding_generation.ipynb  # One-time embedding generation
â”œâ”€â”€ lambda_function.py                # Main Lambda handler
â”œâ”€â”€ products_300.json                 # Product catalog (not included)
â”œâ”€â”€ README.md                         # This file
â””â”€â”€ requirements.txt                  # Python dependencies
```

## ğŸ› ï¸ Tech Stack

- **AWS Lambda**: Serverless compute
- **AWS Bedrock**: AI model access (Titan Embed V2 + Text Express)
- **AWS S3**: Storage for embeddings and metadata
- **AWS API Gateway**: HTTP endpoint
- **NumPy**: Vector operations and cosine similarity
- **Python 3.9**: Runtime environment

## ğŸŒ Use Cases

- **E-commerce Search**: Natural language product search
- **Recommendation Engine**: Context-aware suggestions
- **Price Comparison**: Budget-conscious shopping
- **Multilingual Support**: Arabic/English queries

## ğŸš§ Future Enhancements

- [ ] Add user preference learning
- [ ] Implement conversation history
- [ ] Add image-based search
- [ ] Support for filters (brand, rating, availability)
- [ ] Real-time inventory updates
- [ ] A/B testing for ranking algorithms

## ğŸ“„ License

MIT License - Feel free to use this project for learning or commercial purposes.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**â­ If you find this project helpful, please give it a star!**
